\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,hyperref,mathtools,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\begin{document}

\title{MATH 213 --- Advanced Mathematics for Software Engineers}
\author{Kevin James}
\date{\vspace{-2ex}Spring 2014}
\maketitle\HRule

\tableofcontents
\newpage

\section{Differential Equations}
{\bf Differential equations} are equations involving derivatives with respect to some independant variable. For example, Newton's Law states \[ M \ddot x = F \] or \[ M \dderiv{x}{t} = F \]

In the {\bf classical approach}, we suppose $f$ is given as a function of time, and we solve for the dependant variable with respect to the independant one. For example, \[ F(x) \function x(t) \]

The {\bf systems approach} has less of an emphasis on the response to a specific input and deals more with the overall relationships between the function and between the individual dependant variables.

\subsection{Examples}
The population of an organism (given abundant resources) or the growth of an economy (if the economy were to grow at a constant percentage rate) can be modelled as:
\begin{align*}
\dot x &= ax\\
\frac{\dd x}{x} &= a \dd t\\
\int \frac{\dd x}{x} &= \int a \dd t\\
\ln x + C_1 &= at + C_2\\
\ln x &= at + C_3\\
x(t) &= e^{at + C_3}\\
x(t) &= e^{at} \times e^{C_3}\\
x(t) &= C_4 \times e^{at}\\
x(t) &= x(0) \times e^{at}
\end{align*}
where the value of $x(0)$ is called the {\bf initial condition}.

Given that this function assumes that the population growth is not limited by resources, etc., it is not very useful in the real world. More likely, we would find for large populations a limit of some sort must be included. For example, the {\bf logistic equation} is modelled as:
\begin{align*}
\dot x &= ax - bx^2\\
\frac{\dd x}{ax - bx^2} &= \dd t\\
\int \frac{\dd x}{ax - bx^2} &= \int \dd t\\
\int \frac{\dd x}{x(a - bx)} &= \int \dd t\\
\int \dd x (\frac{A}{x} + \frac{B}{a - bx}) &= \int \dd t\\
\int \dd x (\frac{1}{ax} + \frac{B}{a - bx}) &= \int \dd t\\
\int \dd x (\frac{1}{ax} + \frac{b}{a(a - bx)}) &= \int \dd t\\
\int \frac{\dd x}{ax} + \int \frac{b \dd x}{a(a - bx)} &= \int \dd t\\
\frac{\ln x}{a} + \frac{b}{a}\frac{-1}{b}\ln(a - bx) &= t + C_0\\
\frac{1}{a} \bigg(\ln\frac{x}{a - bx}\bigg) &= t + C_0\\
\frac{x}{a - bx} &= e^{at + aC_0}\\
x &= C_1 e^{at} (a - bx)\\
x &= \frac{aC_1 e^{at}}{1 + bC_1e^{at}}\\
x &= \frac{a}{b} \bigg(\frac{1}{1 + C_2 e^{-at}}\bigg)
\end{align*}
Where $C_2 = \frac{1}{bC_1}$. In this case, the population will ``level out'' at $ax = bx^2$ (i.e.\ have an asymptote). The solution to this specific {\bf DE (differential equation)} is called the {\bf logistic curve}.

\subsection{Partial Differential Equations}
{\bf PDEs (partial differential equations)} arise when there is more than one independant variable.

If we were to model the vibration of a string, we would use a PDE.\@ Assuming there is no length-wise vibration (i.e.\ horizontal motion), that the string has constant tension and mass per unit length, and that we are only considering small transverse displacements, we could write Newton's equation $F = ma$ as \[ F = (\rho\Delta x) \times \dderiv{y}{t} \]

Since all forces on this string are tension, we have \[ F = T\sin\theta_1 - T\sin\theta_2 \]

Given that we have small displacements (which leads to small angles $\theta_1$ and $\theta_2$), we can replace all instances of $\sin$ with $\tan$. Finally, this gives us \[ F \approx T\deriv{y}{x} \bigg|_{x+\frac{\Delta x}{2}} - T\deriv{y}{x} \bigg|_{x-\frac{\Delta x}{2}} \]

so therefore \[ \rho\dderiv{y}{t} = \frac{T}{\Delta x} \bigg(\deriv{y}{x} \bigg|_{x+\frac{\Delta x}{2}} - \deriv{y}{x} \bigg|_{x-\frac{\Delta x}{2}}\bigg) \]

As $\Delta x$ approaches zero, we see that \[ \dderiv{y}{t} = \frac{T}{\rho} \dderiv{y}{X} \]

We will not be solving PDEs in this course, but this equation is solveable to give us \[ y = A\sin k\bigg(x - t\sqrt{\frac{T}{\rho}}\bigg) \]

\subsubsection{Boundary Conditions}
This equation should additionally have several {\bf boundary conditions} which represent the ends of the string being fixed in place.

\begin{align*}
y(0, t) &= 0\\
y(L, t) &= 0
\end{align*}

We use these equations as well as the equation for a standing wave to get \[ y = A_+ \sin k \bigg( x - t\sqrt{\frac{T}{p}} \bigg) + A_- \sin k \bigg( L - t\sqrt{\frac{T}{\rho}} \bigg) \]

Since $y(0, t) = 0$,
\begin{align*}
0 &= A_+ \sin k \bigg( 0 - t\sqrt{\frac{T}{p}} \bigg) + A_- \sin k \bigg( 0 - t\sqrt{\frac{T}{\rho}} \bigg)\\
A_+ &= A_- = A
\end{align*}

and since $y(L, t) = 0$,
\begin{align*}
y &= A_+ \sin k \bigg( L - t\sqrt{\frac{T}{p}} \bigg) + A_- \sin k \bigg( L - t\sqrt{\frac{T}{\rho}} \bigg)\\
KL &= \pm n\pi
\end{align*}

Because waves can only have certain frequences we have
\begin{align*}
K \sqrt\frac{T}{\rho} &= \pm \frac{n\pi}{L} \sqrt\frac{T}{\rho}\\
f &= \pm \frac{n}{2L} \sqrt\frac{T}{\rho}
\end{align*}
where $n = 1$ implies a fundamental frequency and $n \geq 2$ implies a harmonic frequency.

\section{Linear Differential Equations}
A {\bf linear differential equation} has the form: \[ a_n(t)\nderiv{y(t)}{t}{n} + a_{n-1}(t)\nderiv{y(t)}{t}{n-1} + \cdots + a_0(t) y(t) = b_n(t)\nderiv{f(t)}{t}{n} + b_{n-1}(t)\nderiv{f(t)}{t}{n-1} + \cdots + b_0(t) f(t) \]

In this equation, we have two dependant variables: $f$ and $y$. The `classical' approach is to assume $f(t)$ is given, then solve for $y(t)$.

This equation is linear in the sense that it obeys the principle of superposition: if $y_1(t), y_2(t)$ are solutions corresponding to $f_1(t), f_2(t)$, then if $f(t) = k_1f_1(t) + k_2f_2(t)$, then $y(t) = k_1y_1(t) + k_2y_2(t)$.

Given constant coefficients, we would write this equation as \[ a_n\nderiv{y(t)}{t}{n} + a_{n-1}\nderiv{y(t)}{t}{n-1} + \cdots + a_0y(t) = b_n\nderiv{f(t)}{t}{n} + b_{n-1}\nderiv{f(t)}{t}{n-1} + \cdots + b_0f(t) \]

In shorthand, given
\begin{align*}
Dy &= \deriv{y}{t}\\
D^2 y &= \dderiv{y}{t}\\
D^n y &= \nderiv{y}{t}{n}
\end{align*}
we can write this as \[ Q(D)y = P(D)y \] where $Q(x)$ and $P(x)$ are polynomials.

In general, we can assume $P(D) = 1$.

\begin{proof}
Suppose $\tilde y(t)$ is a solution of $Q(D)y = f$ (e.g.\ $Q(D) \tilde y = f$). Now let $y = P(D) \tilde y$. Then
\begin{align*}
Q(D)y &= Q(D)P(D) \tilde y\\
&= P(D)Q(D)\tilde y\\
&= P(D) f
\end{align*}
So $y = Q(D)\tilde y$ solves the original equation.
\end{proof}

\begin{example}
Prove $Dy = (D+1)f$
\end{example}

\begin{proof}
Suppose $Dy = (D + 1)f$ and $f(t) = t, \forall t$. Let's find $\tilde y$ that solves
\begin{align*}
Q(D)\tilde y &= f\\
D\tilde y &= f\\
D\tilde y &= t
\end{align*}
So $\tilde y = \half t^2 + C$. Let
\begin{align*}
y &= P(D)\tilde y\\
&= (D+1)\bigl(\half t^2 + C\bigl)\\
&= t + \half t^2 + C \half
\end{align*}
Then
\begin{align*}
D\bigl(t + \half t^2 + C\bigl) &= 1 + t\\
&= (D + 1)t
\end{align*}
\end{proof}

\begin{definition}
If $f(t)$ is continuous on an interval $a \leq t \leq b$, then there exists a solution $y(t)$ satisfying the above differential equation and also the ``intial conditions'' for $a \leq t_0 \leq b$
\begin{align*}
y(t_0) &= P_0\\
\deriv{y(t_0)}{t} &= P_1\\
\nderiv{y(t_0)}{t}{n-1} &= P_{n-1}
\end{align*}
Moreover, this solution is unique.
\end{definition}

\begin{example}
For a falling block, given the mass and gravitational force, determine the distance fallen over a given time $t$.
\end{example}

\begin{proof}
\begin{align*}
m \ddot y &= mg\\
\ddot y &= g\\
\dot y &= gt + C_1\\
y &= \half gt^2 + C_1t + C_2
\end{align*}
For an $n$-th degree differential equation, we will end up with $n$ unknowns. Thus, we will need $n$ sets of initial conditions to solve for a general solution. Let's assume $y(0) = 0$. This gives us
\begin{align*}
y &= \half gt^2 + C_1t + C_2\\
0 &= C_2\\
\end{align*}
So we now have $y = \half gt^2 + C_1t$. Given a second initial condition $\dot y(0) = 0$, we have
\begin{align*}
\dot y &= gt + C_1\\
0 &= C_1
\end{align*}
Thus \[ y = \half gt^2 \]
\end{proof}

The {\bf general solution} of the equation is an expression for $y$ that solves the equation and contains $n$ arbitrary constants.

Given a differential equation of order $n$ and $n$ initial conditions, we have an {\bf initial value problem}.

To solve such problems, we find the general solution of the differential equation and plug in the initial conditions to evaluate the arbitrary constants.

\begin{example}
We'll first find the general solution of the auxillary equation $q(D)y = 0$ (a ``homogeneous equation'' because the right-hand side is zero). Note: this contains $n$ arbitrary constants and is called the complimentary solution $y_c$.

Then, we'll find any solution of the original equation \[ Q(D)y = f \] This is called a particular solution $y_p$.

To see why this works, let \[ y = y_c + y_p \] Then
\begin{align*}
Q(D)y &= Q(D)(y_c + y_p)\\
&= Q(D)y_c + Q(D)y_p\\
&= 0 + f\\
&= f
\end{align*}
\end{example}

\begin{example}
Prove $(D^2 - 6D + 9) = 0$.

Then
\begin{align*}
Q(m) &= 0\\
m^2 -6m + 9 &= m\\
{(m - 3)}^2 &= 0\\
m &= 3
\end{align*}
where $m$ has multiplicity $2$.
% MISSING

If $y = y_1$ is a solution of $Q(D)y(t) = f(t)$, then substituting $y = y_1v$ yields an equation of order $n=1$ in $\dot v$. Thus we have
\begin{align*}
y_1 &= e^{3t}\\
y &= e^{3t}v\\
Dy &= 3e^{3t}v + e^{3t}\dot v\\
D^2y &= 3(3e^{3t}v + e^{3t}\dot v) + 3e^{3t}\dot v + e^{3t} \ddot v\\
(D^2 - 6D +9)y &= 0\\
9e^{3t} + 6e{3t} \dot v + e^{3t} \ddot v -6 (3e^{3t}v + e^{3t}\dot v) + 9e^{3t} v &= 0\\
e^{3t} \ddot v &= 0\\
\ddot v &= 0\\
\dot v &= c_0\\
v &= c_1t + c_2
\end{align*}
So we have another solution to our original equation \[ y_c = y_1v = e^{3t} (c_1 t + c_2) \]
\end{example}

Generally, if the repeated root $\lambda$ is of multiplicity $k$, then the complimentary solution would be \[ (c_1 + c_2t + \cdots + c_k t^{k-1})e^{\lambda t} \] Of course this implies that with multiplicity $1$ we have \[ ce^{\lambda t} \] The general solution, then, is the given by the sum of all such terms for every $\lambda$ of the characteristic equation.

\begin{example}
For a block of mass $M$ attached to a spring with constant $k$, we have
\begin{align*}
M\ddot y &= ky\\
\ddot y &= \frac{k}{M}y = 0\\
\bigg(D^2 + \frac{k}{M}\bigg)y &= 0\\
Q(m) &= 0\\
M^2 + \frac{k}{M} &= 0\\
m &= \pm i\sqrt\frac{k^2}{M}
\end{align*}

By our method we have
\begin{align*}
y &= c_1 e^{i\sqrt\frac{k}{M} t} + c_2 e^{-i\sqrt\frac{k}{M} t}\\
&= c_1\bigg( \cos\bigl(\sqrt\frac{k}{M} t\bigl) + i\sin\bigl(\sqrt\frac{k}{M} t\bigl)\bigg) + c_2 \bigg(\cos\bigl(-\sqrt\frac{k}{M} t\bigl) + i\sin\bigl(-\sqrt\frac{k}{M} t\bigl)\bigg)
\end{align*}

Consider the potential initial conditions. We would have
\begin{align*}
y(t_0) &= c_1 e^{i\sqrt\frac{k}{M} t_0} + c_2 e^{-i\sqrt\frac{k}{M} t_0}\\
\dot y(t_0) &= i\sqrt\frac{k}{M} \bigg(c_1 e^{i\sqrt\frac{k}{M} t_0} + c_2 e^{-i\sqrt\frac{k}{M} t_0}\bigg)
\end{align*}

So we can find
\begin{align*}
c_1 &= \frac{j\sqrt\frac{k}{M} y(t_0) + \dot y(t_0)}{2i\sqrt\frac{k}{M} e^{i\sqrt\frac{k}{M} t_0}}\\
c_2 &= \frac{i\sqrt\frac{k}{M} y(t_0) - \dot y(t_0)}{2i\sqrt\frac{k}{M} e^{-i\sqrt\frac{k}{M} t_0}}
\end{align*}
or in other words, if $y(t_0)$ and $\dot y(t_0)$ are real-valued, $c_2$ is the complex conjugate of $c_1$ ($c_1 = c_2^*$).

It thus follows that \[ y = 2 |c_1| \cos\bigg(\sqrt\frac{k}{M} t_0 + \angle c_1\bigg) \]
\end{example}

\subsection{Finding Particular Solutions}
The {\bf method of undetermined coefficients} works for functions $f(t)$ of the form of a polynomial, exponential, or a sum of the them. The main idea is to ``guess'' the form of a particular solution. Generally, this solution is of the same form as $f(t)$.

\begin{example}
% CIRCUIT QUESTION EXAMPLE
Suppose $f(t) = t \cdot \forall t$. Then we may guess that we may have $y_p (t) = k t$, which gives us $Dy_p = k$. To satisfy our differential equation, we need
\begin{align*}
(D + 50)y_p &= f = t\\
k + 50kt &= t
\end{align*}
since there is no constant solution for $k$, this can not be true. So we can guess that we have a more complicated equation $y_p (t) = k_0 + k_1 t$. This gives us $Dy_p = k_1$ and we have
\begin{align*}
(D + 50)y_p (t) &= f(t) = t\\
k_1 + 50(k_0 + k_1 t) &= t\\
t &= \frac{1}{50} + 50\bigg(-{\bigg(\frac{1}{50}\bigg)}^2 + \frac{1}{50}t\bigg)
\end{align*}
which gives us $k_0 = -0.0004$ and $k_1 = 0.02$.

So the solution of the equation is
\begin{align*}
y &= y_c + y_p\\
&= ce^{-50t} - 0.0004 + 0.02t
\end{align*}

Suppose an initial condition is given: $y(0) = 0$. So we have
\begin{align*}
y(t) &= ce^{-50t} - 0.0004 + 0.02t\\
0 &= ce^0 - 0.0004 + 0.02(0)\\
&= c - 0.0004\\
c &= 0.0004
\end{align*}
\end{example}

The general method is as follows:
\begin{itemize}
\item for a polynomial function $f(t)$ of degree $d$, assume that the particular solution is of degree $d$ with \emph{undetermined coefficients}: \[ y_p(t) = k_0 + k_1t + \cdots + k_d t^d \]
\item for an exponential function $f(t) = e^{\lambda t}$, assume that a particular solution is \[ ke^{\lambda t} \]
\item for sums of polynomials and exponentials, look for particular solutions which are sums of the above two forms
\item if any of the terms in the particular solution presented by the above rules occur in the complementary solution, multiply that term by the smallest power of $t$ which will give us a value not present in the complementary solution
\end{itemize}

\begin{example}
Given the response of an RC circuit to exponentials \[ (D + 50)y(t) = f(t) = e^{\lambda t} \] we can, by our second rule, try
\begin{align*}
y_p(t) &= ke^{\lambda t}\\
Dy_p &= k\lambda e^{\lambda t}
\end{align*}
and through substitution \[ k\lambda e^{\lambda t} + 50ke^{\lambda t} = x^{\lambda t} \]

So we have
\begin{align*}
k\lambda + 50k &= 1\\
k &= \frac{1}{50+\lambda}
\end{align*}
if and only if $\lambda \neq -50$.  In this case, $y_p(t)$ solves the auxiliary equation $Q(D)y = 0$.

If $\lambda = -100$, $y_p(t) = -\frac{1}{50}e^{\lambda t}$ is a particular solution. But if $\lambda = -50$, then we apply our fourth rule
\begin{align*}
y_p(t) &= kte^{\lambda t}\\
Dy_p(t) &= ke^{\lambda t} + \lambda kte^{\lambda t}
\end{align*}

Substituting, we have
\begin{align*}
Dy_p + 50 y_p(t) &= e^{\lambda t}\\
ke^{\lambda t} - \lambda kte^{\lambda t} + 50kte^{\lambda t} &= e^{\lambda t}\\
\lambda k &= -50\\
k &= 1
\end{align*}

This gives us the general solution \[ y(t) = ce^{-50t} + te^{-50t} \]
\end{example}

Note that our method allows for sinusoidal functions $f(t)$. We have \[ \cos\omega t = \frac{e^{j\omega t} + e^{-j\omega t}}{2} \] and \[ \sin\omega t = \frac{e^{j\omega t} - e^{-j\omega t}}{2j} \]

\begin{example}
If $f(t) = e^{j\omega t}$ (for the previous example), we can apply the second method to get the particular solution \[ y_p(t) = \frac{50}{50 + j\omega} e^{j\omega t} \]

If we instead have $f(t) = e^{-j\omega t}$, we set \[ y_p(t) = \frac{1}{50-j\omega}e^{-j\omega t} \]

So if $f(t) = \cos\omega t$, then, by linearity, we can take \[ y_p(t) = \half \bigg(\frac{1}{50+j\omega}e^{j\omega t} + \frac{1}{50-j\omega} e^{-j\omega t}\bigg) \] If we let $k = \frac{1}{50 + j\omega}$, then we can write
\begin{align*}
y_p(t) &= \half \bigg(ke^{j\omega t} + k^* e^{-j\omega t}\bigg)\\
&= \half \bigg( |k|e^{j\angle k}e^{j\omega t} + |k|e^{-j\angle k}e^{-j\omega t}\bigg)\\
&= |k|\half \bigg(e^{j(\omega t+ \angle k)} + e^{-j(\omega t + \angle k)}\bigg)\\
&= |k| \cos(\omega t + \angle k)
\end{align*}

Note: because $|k| \to 0$ as $\omega \to \infty$, $|k| = \frac{1}{\sqrt{50^2 + \omega^2}} \to $ ``low-pass filter''
\end{example}

\begin{example}
Suppose $f(t) = \sin 2t = \frac{e^{j2t} - e^{-j2t}}{2j}$. What is a particular solution?

\begin{proof}
% MISSING y_c
It suffices to find a particular solution for $f(t) = e^{\pm j2t}$ and then apply superposition.

We have
\begin{align*}
f(t) &= e^{j2t}\\
y_p(t) &= ke^{j2t}\\
Dy_p(t) &= wkje^{j2t}\\
D^2y_p(t) &= -4ke^{j2t}
\end{align*}

We substitute this into
\begin{align*}
\bigg(D^2 + \frac{3}{2}D + \quarter\bigg) y = f &= e^{j2t}\\
\bigg(-4 + 3j + \quarter\bigg) ke^{j2t} &= e^{j2t}\\
\bigg(\frac{-15}{4} + 3j\bigg) ke^{j2t} &= e^{j2t}
\end{align*}
so $k$ is the inverse
\begin{align*}
k &= \frac{1}{(\frac{-15}{4}) + 3j} \frac{(\frac{-15}{4}) + 3j}{(\frac{-15}{4}) + 3j}\\
&= \frac{(\frac{-15}{4}) + 3j}{{(\frac{15}{4})}^2 + 3^2}\\
&= \frac{-20 -16j}{123}
\end{align*}
so we have a particular solution \[ y_p(t) = \frac{-20-16j}{123}e^{j2t} \]

We can also write
\begin{align*}
k &= \frac{-20 - 16j}{123}\\
&= \frac{\sqrt{656}}{123} e^{-j2.47}\\
&= \frac{4}{3\sqrt{41}} e^{-j2.47}\\
&= |k|e^{j\angle k}
\end{align*}
so
\begin{align*}
y_p(t) &= \frac{4}{3\sqrt{41}} e^{-j2.47}e^{j2t}\\
&= \frac{4}{3\sqrt{41}} e^{j(2t-2.47)}
\end{align*}

Then if $f(t) = e^{-j2t}$ we get a particular solution
\begin{align*}
y_p(t) &= k^* e^{-j2t}\\
&= |k^*| e^{-j(2t-\angle k^*)}\\
&= |k| e^{-j(2t+\angle k)}
\end{align*}
i.e.\ \[ y_p(t) = \frac{4}{3\sqrt{41} e^{-j(2t - 2.47)}} \]

So if $f(t) = \sin 2t = \frac{e^{j2t} - e^{-j2t}}{2j}$, then a particular solution is \[ y_p(t) = \frac{|k|e^{j(2t+\angle k)} - |k|e^{-j(2t+\angle k)}}{2j} \] i.e.\
\begin{align*}
y_p(t) &= \frac{4}{3\sqrt{41}} \frac{e^{j(2t-2.47)} - e^{-j(2t-2.47)}}{2j}\\
&= \frac{4}{3\sqrt{41}} \sin(2t-2.47)\\
&= |k|\sin (2t + \angle k)
\end{align*}

So the general solution of $Q(D) \tilde y = f$ is \[ \tilde y = c_1 e^{(-\frac{3}{4} + \frac{\sqrt{5}}{4})t} + c_2 e^{(-\frac{3}{4} - \frac{\sqrt{5}}{4})t} + \frac{4}{3\sqrt{41}} \sin(2t-2.47) \]

So then the general solution of $Q(D)y = P(D)f$ is
\begin{align*}
y &= P(D)\tilde y\\
&= (D+1)\tilde y\\
&= \bigg(\quarter + \frac{\sqrt{5}}{4}\bigg) c_1 e^{(-\frac{3}{4} + \frac{\sqrt{5}}{4})t} + \bigg(\quarter - \frac{\sqrt{5}}{4}\bigg) c_2 e^{(-\frac{3}{4} - \frac{\sqrt{5}}{4})t} + \frac{4}{3}\sqrt{\frac{5}{41}} \bigg(\frac{1}{\sqrt{5}} \sin(2t - 2.47) + \frac{2}{\sqrt{5}} \cos(2t-2.47) \bigg)
\end{align*}

So since
\begin{align*}
&= \frac{1}{\sqrt{5}} \sin(2t-2.47) + \frac{2}{\sqrt{5}} \cos(2t-2.47)\\
&= \sin 0.47\sin(2t-2.47) + \cos 0.47\cos(2t-2.47)\\
&= \sin(2t-2.47+0.47)\\
&= \sin(2t-2)
\end{align*}
we have \[ y = c_3 e^{(\frac{-3}{4} + \frac{\sqrt{5}}{4})t} + c_4 e^{(\frac{-3}{4} - \frac{\sqrt{5}}{4})t} + \frac{4}{3} \sqrt{\frac{5}{41}} \sin(2t-2) \]
\end{proof}

If we were given initial conditions (e.g.\ $y(0)$ and $\dot y(0)$), we could evaluate $c_3$ and $c_4$.
\end{example}

\subsection{Laplace Transforms}
A {\bf Laplace transform} simplifies the process of solving a linear differential equation by ``converting'' it to a standard linear algebra problem.

For example, if all of our functions were exponentials $y = e^{st}$, we would have $Dy = sy$.

Since not all of our functions are exponentials, we follow the process of decomposing our functions into \emph{weighted sums} of exponential equations, then applying standard algebraic practices to these sums. By recomposing these equations afterward, we can find solutions to differential equations with a minimal of effort.

More formally, a Laplace transform is denoted by \[ F(s) := \mathcal{L} \{f(t)\} = \int_{-\infty}^\infty f(t) e^{-st} \dd t \] where $s$ is complex-valued. To ensure convergence, suppose that for some real $\alpha$, the integral \[ \int_{-\infty}^{\infty} |f(t)| e^{-\alpha t} \dd t \] converges for some $s$.

The transform can be inverted by means of the following {\bf inversion integral}: \[ f(t) = \frac{1}{2\pi j} \int_{\sigma-j\infty}^{\sigma+j\infty} F(s) e^{st} \dd s \] This is the {\bf contour integral} which is carried out within the area of the complex plain in which the Laplace transform converges (the ``region of convergence'' of $F(s)$).

\begin{example}
Let $f(t) = 1, \forall t$. Then we can suppose $s = \alpha + j\beta$ for some $\alpha, \beta \in \mathbb{R}$. Thus \[ \int_{-\infty}^\infty f(t)e^{-st} \dd t = \int_{-\infty}^\infty e^{-\alpha t} e^{-j\beta t} \dd t \] does not exist for any value of $\alpha$.
\end{example}

\begin{example}
Let $f(t) = u_{-1}(t)$, where $u$ denotes the unit step function $u(t) =
\begin{dcases*}
1 & if $t \geq 0$\\
0 & otherwise
\end{dcases*}$. Then \[ \int_{-\infty}^\infty f(t)e^{-(\alpha + j\beta)t} \dd t = \int_{-\infty}^\infty e^{-\alpha t} e^{-j\beta t} \dd t \] converges for any $\alpha > 0$.

So we have
\begin{align*}
F(s) &= \int_{-\infty}^\infty f(t) e^{-st} \dd t\\
&= \int_0^\infty e^{-st} \dd t\\
&= \bigg(\frac{-1}{s}\bigg)e^{-st} \limit{0}{\infty}\\
&= 0 - \bigg(\frac{-1}{s}\bigg)
\end{align*}
so \[ F(s) = \frac{1}{s} \text{, given }\Re(s) > 0 \]
\end{example}

\begin{example}
Let
\begin{align*}
f(t) &=
\begin{dcases*}
e^{\alpha t} & if $t \geq 0$\\
0 & otherwise
\end{dcases*}\\
&= e^{\alpha t}u_{-1}(t)
\end{align*}

Then we have
\begin{align*}
F(s) &= \int_0^\infty e^{\alpha t} e^{-st} \dd t\\
&= \int_0^\infty e^{-(s-\alpha)t} \dd t
\end{align*}

So \[ F(s) = \frac{1}{s-\alpha} \text{, given }\Re(s-\alpha) > 0 \iff \Re(s) > \Re(\alpha) \]
\end{example}

\begin{example}
Suppose $f(t) = t u_{-1}(t)$. Then
\begin{align*}
F(s) &= \int_0^\infty t e^{-st} \dd t\\
&= -\frac{1}{s}t e^{-st} \limit{0}{\infty} - \int_0^\infty \bigg(\frac{-1}{s}\bigg) e^{-st} \dd t\\
&= 0 - 0 + \frac{1}{s} \int_0^\infty e^{-st} \dd t\\
&= \frac{1}{s^2} \text{, given }\Re(s) > 0
\end{align*}
\end{example}

We'll deal mainly with ``one-sided'' functions $F(t)$ for which $F(t) = 0, \forall t < 0$. We'll therefore often use the ``one-sided'' Laplace transform \[ F(s) = \int_{0^-}^\infty f(t) e^{-st} \dd t = \lim_{m\uparrow 0} \int_m^\infty f(t) e^{-st} \dd t \]

\begin{example}
Suppose $f(t) = \sin(\omega t) e_{-1}(t), \omega \in \mathbb{R}$. Then $\sin{\omega t} = \frac{e^{j\omega t} - e^{-j\omega t}}{2j}$ and so
\begin{align*}
F(s) &= \frac{1}{2j} \bigg[\frac{1}{s-j\omega} - \frac{1}{s+j\omega}\bigg]\\
&= \frac{1}{2j} \bigg[\frac{(s+j\omega) - (s - j\omega)}{(s-j\omega)(s + j\omega)}\bigg]\\
&= \frac{\omega}{s^2 + \omega^2}\text{, given }\Re(s) > 0
\end{align*}
\end{example}

The Laplace transformation obeys the following key properties:
\begin{itemize}
\item Linearity: $\mathcal{L} \{ \alpha f(t) + \beta g(t) \} = \alpha F(s) + \beta G(s)$
\item Time-Scaling: $\mathcal{L} \{f(ct)\} = \frac{1}{c}F(\frac{s}{c})$
\item Exponential Modulation: $\mathcal{L} \{e^{\alpha t} f(t)\} = F(s-\alpha)$
\item Time Shifting: Suppose $F(s) = \mathcal(L) \{ f(t) u_{-1}(t) \}$ and let $g(t) = f(t - T)u_{-1}(t - T)$. Then $G(S) = \mathcal{L} \{g(t)\} = e^{-sT} F(s)$
\item Multiplication by $t$: $\mathcal{L} \{ tf(t) \} = -\frac{\dd}{\dd s} F(s)$
\item Differentiation/Integration: Suppose that there exists a real $\alpha$ such that $\displaystyle\int_{0^-}^\infty |f(t)| e^{-\alpha t} \dd t$ converges. Suppose also that there exists a function $f^\prime(t)$ such that $f(t) = f(0^-) + \displaystyle\int_{0^-}^\infty f^\prime(\tau) \dd \tau, \forall t \geq 0$ and that $\displaystyle\int_{0^-}^\infty |f^\prime(t)| e^{-\beta t} \dd t$ converges for some real $\beta$. Then both $f$ and $F^\prime$ have one-sided Laplace transforms and we have $F(s) = \frac{1}{s}f(0^-) + \frac{1}{s} \mathcal{L} \{f^\prime(t) \}$ or $\mathcal{L} \{ f^\prime(t) \} = sF(s) - f(0^-)$
\end{itemize}

\subsubsection{Solving Differential Equations with Laplace Transforms}
To solve a differential equation, we may simply take the Laplace transform of each side of the equation, solve for the value of $F(s)$, and then perform the inverse Laplace transformation on each side.

\begin{example}
Assume we have $\dot y + y = t + e^t$ with the initial condition $y(0^-) = 1$.

If we take the Laplace transform of each side of this equation, we have \[ sY(s) - y(0^-) + Y(s) = \frac{1}{s^2} + \frac{1}{s-1} \]

This is a much simpler equation to solve, and can in fact be solved using only standard algebra techniques.

We have
\begin{align*}
sY(s) - y(0^-) + Y(s) &= \frac{1}{s^2} + \frac{1}{s-1}\\
sY(s) - 1 + Y(s) &= \frac{1}{s^2} + \frac{1}{s-1}\\
(s+1)Y(S) &= 1 + \frac{1}{s^2} + \frac{1}{s-1}\\
Y(s) &= \frac{1}{s+1} + \frac{1}{s^2(s+1)} + \frac{1}{(s-1)(s+1)}\\
&= \frac{\frac{3}{2}}{s+1} + \frac{1}{s^2} - \frac{1}{s} + \frac{\frac{1}{2}}{s-1}\\
y(t) &= \frac{3}{2}e^{-t} + t - 1 + \half e^t \text{, given }t \geq 0
\end{align*}
\end{example}

\subsubsection{Inversion Integral}
We know that $\mathcal{L} ^{-1} \{ F(s) + G(s) \} = f(t) + g(t)$. Thus we have \[ \mathcal{L}^{-1} \{ F(s)G(s) \} = (f * g)(t) = \int_{-\infty}^\infty f(\tau)g(t-\tau) \dd\tau \] which is the {\bf convolution} of $f$ and $g$. Given $u = t - \tau$, we can see that this operation is commutative (e.g.\ the convolution of $f$ and $g$ is equivalent to the convolution of $g$ and $f$).

Consider two ``discrete-time'' functions $f,g : \mathbb{Z} \to \mathbb{C}$ that are nonzero only for a finite number of nonnegative integers. We can think of $f$ and $g$ as representations of two polynomials \[ p(x) = \sum_{n=-\infty}^\infty f(n)x^n \] and \[ q(x) = \sum_{n=-\infty}^\infty g(n) x^n \]

Then the convolution $f * g$ represents the product of $p$ and $q$.

Suppose that, for some real $\alpha, \beta$, the integrals \[ \int_{-\infty}^\infty |f(t)| e^{-\alpha t} \dd t \] and \[ \int_{-\infty}^\infty |g(t)| e^{-\beta t} \dd t \] converge. We'll show that this means that $(f * g)(t)$ has a transform \[ \mathcal{L} \{ (f*g)(t) \} = F(s)G(s) \]

Suppose that $\gamma \geq \alpha, \beta$ and consider the product of the two integrals
\begin{align*}
&\int_{-\infty}^\infty |f(t)| e^{-\gamma t} \dd t \int_{-\infty}^\infty |g(\tau)| e^{-\gamma \tau} \dd \tau\\
&= \int_{-\infty}^\infty \int_{-\infty}^\infty |f(t)|e^{-\gamma t} |g(\tau)|e^{-\gamma \tau} \dd t \dd \tau\\
&= \int_{-\infty}^\infty\int_{-\infty}^\infty |f(u-\tau)|e^{-\gamma(u-\tau)} |g(\tau)|e^{-\gamma \tau} \dd u \dd \tau\\
&= \int_{-\infty}^\infty \int_{-\infty}^\infty |f(u-\tau)|e^{-\gamma(u-\tau)} |g(\tau)| e^{-\gamma \tau} \dd \tau \dd u\\
&= \int_{-\infty}^\infty \int_{-\infty}^\infty |f(u-\tau)| |g(\tau)| \dd \tau e^{-\gamma t} \dd u \\
&= \int_{-\infty}^\infty \int_{-\infty}^\infty |f(u-\tau) g(\tau)| \dd \tau e^{-\gamma u} \dd u\\
&\geq \int_{-\infty}^\infty \bigg| \int_{-\infty}^\infty f(u-\tau)g(\tau) \dd \tau \bigg| e^{-\gamma u} \dd u
\end{align*}
so the last integral converges and $(f * g)$ has a Laplace transform.

\subsubsection{Initial-Value Theorem}

\begin{definition}
A function $f(t)$ is piecewise-continuous on an interval $[a,b] (\{t: a \leq t \leq b \})$ if $f$ is continuous and bounded everywhere in this interval, except possibly at some finite number of points.

Moreover, at any discontinuity $t_0$, the limits \[ f(t_0^-) = \lim_{t\uparrow t_0} f(t) \] and \[ f(t_0^+) = \lim_{t\downarrow t_0} f(t) \] must exist.

If $f$ is piecewise-continuous in all such intervals, then $f$ is piecewise-continuous.
\end{definition}

\begin{theorem}[The Initial-Value Theorem]
If $f$ is piecewise-continuous and there exists a real $\alpha$ such that \[ \int_{-\infty}^\infty |f(t)| e^{-\alpha t} \dd t\] converges, then \[ f(0^+) = \lim_{s\to\infty} sF(s) \]

This gives a means of finding $f(0^+)$ from $F(s)$ withouts inverting the transform.
\end{theorem}

This limit shows us that the real part of $s$ trends to $+\infty$.

\begin{proof}
\begin{align*}
\lim_{s\to\infty} sF(s) &= \lim_{s\to\infty} s\int_{0^-}^\infty f(t) e^{-st} \dd t\\
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(t) e^{-st} \dd t + \lim_{s\to\infty} s\int_\epsilon^\infty f(t) e^{-st} \dd t\\
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(t) e^{-st} \dd t + \lim_{s\to\infty} \int_\epsilon^\infty f(t) se^{-st} \dd t\\
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(t) e^{-st} \dd t + \int_\epsilon^\infty \lim_{s\to\infty} f(t) se^{-st} \dd t\\
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(t) e^{-st} \dd t
\end{align*}
As $\epsilon$ approaches zero, this approaches
\begin{align*}
&= \lim_{s\to\infty} s\int_{0^-}^\epsilon f(0^+) e^{-st} \dd t\\
&= \lim_{s\to\infty} s f(0^+) \bigg[ \frac{1-e^{-s\epsilon}}{s} \bigg]\\
&= \lim_{s\to\infty} f(0^+) (1-e^{-s\epsilon})\\
&= f(0^+)
\end{align*}
\end{proof}

\subsubsection{Rational Functions}
Many of the Laplace transforms we've seen are rational functions (functions represented as ratios of polynomials). As with rational numbers, we generally cancel out common factors in the numerator and denominator.

Moreover, just as the rational numbers extend the integers to a field, so too do the rational functions extend the polynomials to a field.

The roots of the numerator are called the (finite) zeroes of the function; those of the denominator are the (finite) poles.

A function which has no finite zeroes such as $\frac{1}{s^2}$ is said to have zeroes at infinity, since in the theory of functions of a complex variable they tend to zero as $s\to\infty$. The reciprocal of this type of function is said to have poles at infinity.

If we don't specify finite or infinite, we tend to be refering to finite functions.

A rational function is {\bf proper} if the degree of the numerator is no greater than that of the denominator. If the degree of the numerator is strictly less than that of the denominator, the funtion is strictly proper.

\subsubsection{Final-Value Theorem}
Let $F(s)$ be a proper rational functions, all of whose poles have real parts that are strictly negative, with the possible expection of a single pole at $s = 0$. Alternatively, $F(s)$ may consist of such a function multiplied by an exponential. Then \[ \lim_{s\to\infty} f(t) = \lim_{s\to\infty} sF(s) \] Moreover, if the poles of $F(s)$ do not satisfy the above assumption, then the limit on the left side does not exist.

\end{document}
