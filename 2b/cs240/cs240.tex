\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,parskip,custom}
\usepackage[margin=1in]{geometry}
\begin{document}

\title{CS 240 --- Data Structures and Data Management}
\author{Kevin James}
\date{\vspace{-2ex}Spring 2014}
\maketitle\HRule

\section{Algorithms}
An {\bf algorithm} is a step-by-step process for carrying out a set of operations given an arbitrary problem instance. An algorithm {\bf solves} a problem if, for every instance of the problem, the algorithm finds a valid solution in finite time.

A {\bf program} is an implementation of an algorithm using a specified programming language.

For each problem we can have several algorithms and for each algorithm we can have several programs (implementations).

In practice, given a problem:
\begin{enumerate}
\item Design an algorithm.
\item Assess the correctness of that algorithm.
\item If the algorithm is acceptable, implement it. Otherwise, return to step 1.
\end{enumerate}

When determining the efficiency of algorithms, we tend to be primarily concerned with either the runtime or the memory requirements. In this course, we will focus mostly on the runtime.

To perform runtime analysis, we may simply implement the algorithm and use some method to determine the end-to-end time of the program. Unfortunately, this approach has many variables: test system, programming language, programmer skill, compiler choice, input selection, \dots. This, of course, makes manual implementation a bad approach.

An idealized implementation uses a {\bf Random Access Machine (RAM)} model. RAM systems have constant time access to memory locations and constant time primitive operations, thus the running time is determinable (as the number of memory operations plus the number of primitive operations).

We can also generally use {\bf order notation} to compare multiple algorithms. For the most part, we compare assuming $n$ is very large, since for small values of $n$ the runtime will be miniscule regardless of algorithm.

We denote the runtime of a function as $T(f(x))$, for example: $T(3 \times 4)$ may be equal to $0.8ns = 8\text{ops}$. The return value is the number of operations required in the worst-case scenario.

Example: given $T_A(n) = 1 000 000n + 2 000 000 000$ and $T_B(n) = 0.01n^2$, which is `better'? For $n < 100 000 000$, algorithm $B$ is better. Since we only care about large inputs, though, we say $A$ is better overall.

\begin{example}
Prove that $2010n^2 + 1388 = \mathbb{O}(n^3)$.
\end{example}

\begin{proof}
$\forall c > 0$, $2010 n^2 + 1388 \leq cn^3$\\
$n > 1388 \implies 2010n^2 + 1388 \leq 2011n^2 \leq cn^3$\\
$2011n^2 \leq cn^3 \iff 2011 \leq cn$\\
$n > \frac{2011}{c} = n_0$
\end{proof}

\begin{definition}
$f(n) = \mathbb{O}(g(n))$ if there exists a positive real number $c$ and an integer $n_0 > 0$ such that $\forall n \geq n_0$, $f(n) \leq cg(n)$.
\end{definition}

More concretely, we can say that $f(n) = \mathbb{O}(af(n))$ and $a^\prime f(n) = \mathbb{O}(f(n))$. It's also worth noting that order notation is transitive (e.g.\ $f(n) = \mathbb{O}(g(n))$ and $g(n) = \mathbb{O}(h(n))$ implies $f(n) = \mathbb{O}(h(n))$).

We use five different symbols to denote order notation:
\begin{itemize}
\item $o$ denotes a function \emph{always less} than a given order
\item $\mathbb{O}$ denotes a function \emph{less than or equal} to a given order
\item $\Theta$ denotes a function \emph{exactly equal} to a given order
\item $\Omega$ denotes a function \emph{greater than or equal} to a given order
\item $\omega$ denotes a function \emph{always greater} than a given order
\end{itemize}

\begin{example}
For the psuedo-function
\begin{verbatim}
function(n):
    sum = 0
    for i=1 to n:
        for j=i to n:
            sum = sum + (i-j)^2
            sum = sum^2
    return sum
\end{verbatim}
we find the order equation
\begin{align*}
&= \Theta(1) + \sum_{i=1}^n \sum_{j=i}^n \Theta(1) + \Theta(1)\\
&= \Theta(1) \sum_{i=1}^n \sum_{j=1}^n 1\\
&= \Theta(1) \sum_{i=1}^n (n-i+1)\\
&= \Theta(1) \bigg( \sum_{i=1}^n n - \sum_{i=1}^n i + \sum_{i=1}^n 1 \bigg)\\
&= \Theta(1) \bigl( n^2 + i^n + n \bigl)\\
&= \Theta(n^2) + \Theta(i^n) + \Theta(n)
\end{align*}
\end{example}

\begin{example}
For the psuedo-function
\begin{verbatim}
function(A,n):
    max = 0
    for i=1 to n:
        for j=i to n:
        sum = 0
        for k=1 to j:
        	sum = A[k]
        	if sum > max:
        		max = sum
    return max
\end{verbatim}
we find the order equation
\begin{align*}
&= \sum_{i=1}^n \sum_{j=i}^n \bigl( 1 + \sum_{k=i}^j c \bigl)\\
&= \sum_{i=1}^n \sum_{j=i}^n c(j-i+1)\\
&= \sum_{i=1}^n \sum_{j=1}^{n-i+1} j\\
&= \sum_{i=1}^n \Theta(n-i+1)\\
&= \sum_{i=1}^n \Theta(i)
\end{align*}
\end{example}

\begin{example}
For the psuedo-function
\begin{verbatim}
function(n):
    sum = 0
    for i=1 to n:
    	j = i
        while j >= 1:
        	sum = sum + i/j
        	j = j/2
    return sum
\end{verbatim}
we find the order equation
\begin{align*}
\sum_{i=1}^n \sum_{j=1}^{\log_2 i} c &= \sum_{i=1}^n (c \log_2 i)\\
&= c\bigl(\log 1 + \log 2 + \log 3 + \cdots + \log n\bigl)\\
\text{all }n\text{ of our terms are below }\log n &\hspace{40pt} \text{half of our }n\text{ terms are above }\frac{n}{2}\\
= \mathbb{O}(n\log n) &\hspace{40pt}= \Omega(\frac{n}{2}\log\frac{n}{2})\\
&\hspace{40pt}= \Omega(n\log n)\\
&= \Theta(n\log n)
\end{align*}
\end{example}

\section{Dynamic Arrays}
Linked lists support $\mathbb{O}(1)$ insertion and deletion, $\mathbb{O}(n)$ accessing. Arrays are vice-versa.

{\bf Dynamic arrays} offer a compromise: $\mathbb{O}(1)$ for both, but can only insert or delete from the end of the list.

\end{document}
